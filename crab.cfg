[CRAB]
jobtype = cmssw
scheduler = remoteglidein

[CMSSW]
datasetpath=/HT/Run2012A-22Jan2013-v1/AOD
pset=ntupler_cfg.py

### Splitting parameters
total_number_of_lumis=-1
number_of_jobs = 1000

### Output files
output_file = Run2012A_HT750_NTuple.root

[USER]
### OUTPUT files Management
##  output back into UI
return_data = 1

#user_remote_dir = /eos/uscms/3DayLifetime/bafranci/RUN2012A_Ntuple
#ui_working_dir = /eos/uscms/3DayLifetime/bafranci/RUN2012A_Ntuple
user_remote_dir = /eos/uscms/store/user/bafranci/RUN2012A_Ntuple
ui_working_dir = /eos/uscms/store/user/bafranci/RUN2012A_Ntuple

### OUTPUT files INTO A SE
copy_data = 0

### To publish produced output in a local istance of DBS set publish_data = 1
publish_data=0
### Specify the dataset name. The full path will be <primarydataset>/<publish_data_name>/USER
publish_data_name = name_you_prefer
### Specify the URL of DBS istance where CRAB has to publish the output files
#dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_caf_analysis_01_writer/servlet/DBSServlet


[GRID]
### RB/WMS management:
rb = CERN

##  Black and White Lists management:
## By Storage
se_black_list = T0,T1
#se_white_list =

## By ComputingElement
#ce_black_list =
#ce_white_list =

[CONDORG]
# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor
# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))
